<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Minh h·ªça t∆∞∆°ng t√°c - To√°n h·ªçc c∆° b·∫£n v√† Machine Learning</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 30px;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            color: #ffffff;
            min-height: 100vh;
            font-size: 18px;
            line-height: 1.8;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 30px;
            backdrop-filter: blur(10px);
        }
        h1 {
            text-align: center;
            margin-bottom: 40px;
            color: #ffffff;
            text-shadow: 3px 3px 6px rgba(0,0,0,0.8);
            font-size: 3.5em;
            font-weight: 900;
            letter-spacing: 2px;
        }
        .subtitle {
            text-align: center;
            margin-bottom: 50px;
            font-size: 1.8em;
            opacity: 1;
            font-weight: 600;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.7);
        }
        .illustrations-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px;
            margin-top: 40px;
        }
        .illustration-card {
            background: rgba(255, 255, 255, 0.15);
            border-radius: 15px;
            padding: 25px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            cursor: pointer;
        }
        .illustration-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
        }
        .card-icon {
            font-size: 3em;
            text-align: center;
            margin-bottom: 15px;
        }
        .card-title {
            font-size: 1.8em;
            font-weight: 800;
            margin-bottom: 15px;
            text-align: center;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.6);
            line-height: 1.3;
        }
        .card-description {
            text-align: center;
            margin-bottom: 25px;
            opacity: 1;
            line-height: 1.7;
            font-size: 1.1em;
            font-weight: 500;
        }
        .card-topics {
            background: rgba(255, 255, 255, 0.1);
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        .card-topics h4 {
            margin: 0 0 15px 0;
            font-size: 1.4em;
            font-weight: 700;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
        }
        .card-topics ul {
            margin: 0;
            padding-left: 25px;
            font-size: 1.1em;
            font-weight: 500;
        }
        .card-topics li {
            margin-bottom: 10px;
            line-height: 1.6;
        }
        .open-button {
            background: linear-gradient(45deg, #ff4757, #ff3742);
            color: white;
            border: none;
            padding: 18px 30px;
            border-radius: 12px;
            cursor: pointer;
            font-size: 1.3em;
            font-weight: 800;
            width: 100%;
            transition: all 0.3s ease;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
            box-shadow: 0 4px 15px rgba(255, 71, 87, 0.4);
        }
        .open-button:hover {
            transform: scale(1.02);
        }
        .intro-section {
            background: rgba(255, 255, 255, 0.1);
            padding: 25px;
            border-radius: 15px;
            margin-bottom: 40px;
            text-align: center;
        }
        .intro-section h2 {
            margin-bottom: 15px;
            color: #fff;
        }
        .intro-section p {
            line-height: 1.6;
            opacity: 0.9;
        }
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 30px;
        }
        .feature {
            text-align: center;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
        }
        .feature-icon {
            font-size: 2em;
            margin-bottom: 10px;
        }
        .feature h3 {
            margin-bottom: 10px;
        }
        .topic-section {
            margin: 50px 0;
        }
        .topic-header {
            text-align: center;
            margin: 40px 0 30px 0;
            padding: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
        }
        .topic-header h2 {
            margin: 0 0 15px 0;
            font-size: 2.4em;
            color: #ffffff;
            font-weight: 800;
            text-shadow: 3px 3px 6px rgba(0,0,0,0.7);
        }
        .topic-header p {
            margin: 0;
            opacity: 1;
            font-size: 1.3em;
            font-weight: 500;
            line-height: 1.7;
        }
    </style>



</head>
<body>
    <div class="container">
        <h1>üéì Minh h·ªça t∆∞∆°ng t√°c</h1>
        <div class="subtitle">
            To√°n h·ªçc c∆° b·∫£n v√† Machine Learning cho sinh vi√™n IT
        </div>
        
        <div class="intro-section">
            <h2>üöÄ Roadmap Data Science th√¥ng qua To√°n h·ªçc ·ª®ng d·ª•ng</h2>
            <p>
                <strong>T·∫°i sao l·∫°i h·ªçc to√°n cho Data Science?</strong> B·ªüi v√¨ math IS the language of AI! ü§ñ 
                M·ªói algorithm b·∫°n s·ª≠ d·ª•ng (linear regression, neural networks, random forests) ƒë·ªÅu c√≥ mathematical foundations. 
                Hi·ªÉu to√°n = hi·ªÉu WHY algorithms work, WHEN to use them, v√† HOW to improve them. 
                ƒê√¢y kh√¥ng ch·ªâ l√† theory - ƒë√¢y l√† practical toolkit ƒë·ªÉ tr·ªü th√†nh world-class data scientist!
            </p>
            
            <div class="features">
                <div class="feature">
                    <div class="feature-icon">üéØ</div>
                    <h3>Industry-Ready Learning</h3>
                    <p>M·ªói concept ƒë∆∞·ª£c k·∫øt n·ªëi v·ªõi real-world ML applications: Netflix recommendations, Google search, Tesla autopilot</p>
                </div>
                <div class="feature">
                    <div class="feature-icon">üßÆ</div>
                    <h3>Interactive Mastery</h3>
                    <p>Hands-on exploration v·ªõi sliders, visualizations, v√† real-time computations - learn by doing!</p>
                </div>
                <div class="feature">
                    <div class="feature-icon">üìñ</div>
                    <h3>Business Context</h3>
                    <p>Hi·ªÉu impact c·ªßa mathematical decisions l√™n business outcomes v√† model performance</p>
                </div>
            </div>
        </div>
        
        <!-- LINEAR ALGEBRA SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üî¢ B√†i 1: ƒê·∫°i S·ªë Tuy·∫øn T√≠nh trong Data Science</h2>
                <p>üéØ <strong>T·∫°i sao quan tr·ªçng?</strong> ƒê·∫°i s·ªë tuy·∫øn t√≠nh l√† x∆∞∆°ng s·ªëng c·ªßa Machine Learning! T·ª´ vi·ªác bi·ªÉu di·ªÖn d·ªØ li·ªáu d∆∞·ªõi d·∫°ng ma tr·∫≠n, PCA gi·∫£m chi·ªÅu, ƒë·∫øn neural networks - t·∫•t c·∫£ ƒë·ªÅu d·ª±a tr√™n linear algebra. Vector spaces gi√∫p ta hi·ªÉu kh√¥ng gian features, ma tr·∫≠n transformations m√¥ ph·ªèng c√°c ph√©p bi·∫øn ƒë·ªïi d·ªØ li·ªáu trong deep learning.</p>
            </div>
            <div class="illustrations-grid">
                <div class="illustration-card" onclick="openIllustration('linear-algebra/vector_illustration.html')">
                    <div class="card-icon">üéØ</div>
                    <div class="card-title">Vector v√† Vector Space - N·ªÅn t·∫£ng c·ªßa Features</div>
                    <div class="card-description">
                        üî• <strong>ML Connection:</strong> M·ªói data point l√† 1 vector trong kh√¥ng gian features! V√≠ d·ª•: ·∫£nh 28x28 pixel = vector 784 chi·ªÅu. Dot product t√≠nh similarity gi·ªØa c√°c documents trong NLP, cosine similarity cho recommendation systems.
                    </div>
                    <div class="card-topics">
                        <h4>üß† ·ª®ng d·ª•ng trong ML:</h4>
                        <ul>
                            <li><strong>Feature vectors:</strong> Bi·ªÉu di·ªÖn data points (age, income) ‚Üí (25, 50000)</li>
                            <li><strong>Cosine similarity:</strong> So s√°nh documents, user preferences</li>
                            <li><strong>Distance metrics:</strong> K-means clustering, KNN classification</li>
                            <li><strong>Word embeddings:</strong> Word2Vec, GloVe representations</li>
                        </ul>
                    </div>
                    <button class="open-button">Kh√°m ph√° Vector Space üöÄ</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-algebra/matrix_illustration.html')">
                    <div class="card-icon">üî¢</div>
                    <div class="card-title">Ma Tr·∫≠n - Tr√°i tim c·ªßa Deep Learning</div>
                    <div class="card-description">
                        üí° <strong>Th·ª±c t·∫ø:</strong> M·ªói layer trong neural network l√† 1 ph√©p nh√¢n ma tr·∫≠n! Dataset v·ªõi 1000 samples, 50 features = ma tr·∫≠n 1000√ó50. Weights v√† biases c≈©ng l√† ma tr·∫≠n. Hi·ªÉu matrix operations = hi·ªÉu c√°ch neural networks ho·∫°t ƒë·ªông!
                    </div>
                    <div class="card-topics">
                        <h4>üéØ Applications trong AI/ML:</h4>
                        <ul>
                            <li><strong>Dataset representation:</strong> Rows = samples, Columns = features</li>
                            <li><strong>Neural networks:</strong> Forward pass = chu·ªói matrix multiplications</li>
                            <li><strong>Covariance matrix:</strong> PCA, feature correlation analysis</li>
                            <li><strong>Transformation matrices:</strong> Data augmentation, computer vision</li>
                        </ul>
                    </div>
                    <button class="open-button">Th·ª±c h√†nh Matrix Ops üßÆ</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-algebra/cannon_algorithm_illustration.html')">
                    <div class="card-icon">üßÆ</div>
                    <div class="card-title">Cannon's Algorithm - Distributed AI Training</div>
                    <div class="card-description">
                        ‚ö° <strong>Big Data Reality:</strong> Training GPT models c·∫ßn nh√¢n ma tr·∫≠n kh·ªïng l·ªì (billions parameters)! Cannon's algorithm gi√∫p ph√¢n t√°n computation tr√™n nhi·ªÅu GPUs. V√≠ d·ª•: BERT training tr√™n 64 TPUs, m·ªói TPU x·ª≠ l√Ω 1 ph·∫ßn c·ªßa ma tr·∫≠n weights.
                    </div>
                    <div class="card-topics">
                        <h4>üåê Distributed ML Applications:</h4>
                        <ul>
                            <li><strong>GPU cluster training:</strong> Ph√¢n t√°n matrix multiplication cho large models</li>
                            <li><strong>MapReduce paradigm:</strong> Big data processing, Spark MLlib</li>
                            <li><strong>Federated learning:</strong> Training across multiple devices</li>
                            <li><strong>Model parallelism:</strong> Different GPUs handle different layers</li>
                        </ul>
                    </div>
                    <button class="open-button">T√¨m hi·ªÉu Parallel Computing üåê</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-algebra/strassen_algorithm_illustration.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">Strassen's Algorithm</div>
                    <div class="card-description">
                        Minh h·ªça recursive matrix multiplication v·ªõi Strassen's Algorithm
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>7 multiplications vs 8</li>
                            <li>Recursive decomposition</li>
                            <li>O(n^2.807) complexity</li>
                            <li>Performance comparison</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-algebra/matrix_transformations.html')">
                    <div class="card-icon">üîÑ</div>
                    <div class="card-title">Matrix Transformations</div>
                    <div class="card-description">
                        Minh h·ªça m·ªëi quan h·ªá gi·ªØa ma tr·∫≠n v√† c√°c ph√©p bi·∫øn ƒë·ªïi h√¨nh h·ªçc
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Scale, rotation, shear, reflection transformations</li>
                            <li>Matrix representation of geometric operations</li>
                            <li>Determinant and transformation properties</li>
                            <li>Interactive vector transformation</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-algebra/eigenvalues_eigenvectors.html')">
                    <div class="card-icon">üîç</div>
                    <div class="card-title">Eigenvalues & Eigenvectors - Linh h·ªìn c·ªßa PCA</div>
                    <div class="card-description">
                        üéØ <strong>PCA Magic:</strong> Principal Component Analysis = t√¨m eigenvectors c·ªßa covariance matrix! Eigenvectors ch·ªâ ra directions v·ªõi variance cao nh·∫•t trong data. Netflix recommendations, face recognition ƒë·ªÅu d√πng PCA ƒë·ªÉ gi·∫£m dimensions t·ª´ 1000s features xu·ªëng v√†i ch·ª•c!
                    </div>
                    <div class="card-topics">
                        <h4>üî• ML Breakthroughs v·ªõi Eigenanalysis:</h4>
                        <ul>
                            <li><strong>PCA:</strong> Dimensionality reduction, data visualization, noise removal</li>
                            <li><strong>PageRank algorithm:</strong> Google's ranking system d·ª±a tr√™n dominant eigenvector</li>
                            <li><strong>Spectral clustering:</strong> Graph-based clustering s·ª≠ d·ª•ng eigenvalues</li>
                            <li><strong>Stability analysis:</strong> Neural network training stability</li>
                        </ul>
                    </div>
                    <button class="open-button">Kh√°m ph√° PCA Power üéØ</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-algebra/svd_visualization.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">SVD - Netflix's Billion Dollar Algorithm</div>
                    <div class="card-description">
                        üí∞ <strong>Industry Impact:</strong> SVD l√† tr√°i tim c·ªßa collaborative filtering! Netflix Prize (2009) - $1M gi·∫£i th∆∞·ªüng cho recommendation system. SVD ph√¢n t√°ch user-movie matrix th√†nh latent factors, t√¨m ra hidden patterns. Amazon, Spotify ƒë·ªÅu d√πng SVD variants!
                    </div>
                    <div class="card-topics">
                        <h4>üíé SVD trong Data Science:</h4>
                        <ul>
                            <li><strong>Recommender systems:</strong> Matrix factorization, collaborative filtering</li>
                            <li><strong>Image compression:</strong> Ch·ªâ gi·ªØ top-k singular values, gi·∫£m file size</li>
                            <li><strong>Latent Semantic Analysis:</strong> NLP, document similarity, topic modeling</li>
                            <li><strong>Data imputation:</strong> Fill missing values trong incomplete datasets</li>
                        </ul>
                    </div>
                    <button class="open-button">Master SVD Magic üíé</button>
                </div>
            </div>
        </div>
        
        <!-- CALCULUS SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üìà B√†i 2: Gi·∫£i T√≠ch - Ng√¥n ng·ªØ c·ªßa AI Training</h2>
                <p>üöÄ <strong>Breakthrough Insight:</strong> M·ªçi AI model ƒë·ªÅu "h·ªçc" th√¥ng qua calculus! Gradient descent = ƒëi theo ƒë·∫°o h√†m ƒë·ªÉ minimize loss function. Backpropagation = chain rule ·ª©ng d·ª•ng. Hi·ªÉu calculus = hi·ªÉu c√°ch m√°y "suy nghƒ©" v√† c·∫£i thi·ªán performance!</p>
            </div>
            <div class="illustrations-grid">
                <div class="illustration-card" onclick="openIllustration('calculus/gradient_illustration.html')">
                    <div class="card-icon">üìà</div>
                    <div class="card-title">Gradient - GPS c·ªßa Neural Networks</div>
                    <div class="card-description">
                        üß≠ <strong>The Learning Direction:</strong> Gradient = h∆∞·ªõng tƒÉng nhanh nh·∫•t c·ªßa loss function! Neural networks d√πng -gradient ƒë·ªÉ "ƒëi xu·ªëng n√∫i" loss landscape. V√≠ d·ª•: Training ResNet, gradient magnitude cho bi·∫øt li·ªáu model c√≥ b·ªã vanishing/exploding gradients kh√¥ng.
                    </div>
                    <div class="card-topics">
                        <h4>üéØ Gradient trong Deep Learning:</h4>
                        <ul>
                            <li><strong>Gradient Descent:</strong> Optimizer c∆° b·∫£n - SGD, Adam, RMSprop</li>
                            <li><strong>Backpropagation:</strong> T√≠nh gradient qua chain rule t·ª´ output v·ªÅ input</li>
                            <li><strong>Gradient clipping:</strong> Gi·∫£i quy·∫øt exploding gradients problem</li>
                            <li><strong>Saliency maps:</strong> Visualize what CNN "nh√¨n" th·∫•y trong images</li>
                        </ul>
                    </div>
                    <button class="open-button">Navigate Loss Landscape üß≠</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('calculus/differentiation.html')">
                    <div class="card-icon">üìà</div>
                    <div class="card-title">Interactive Differentiation</div>
                    <div class="card-description">
                        Minh h·ªça t∆∞∆°ng t√°c ƒë·∫°o h√†m v·ªõi nhi·ªÅu h√†m s·ªë v√† ch·∫ø ƒë·ªô xem kh√°c nhau
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Multiple function types (sin, cos, polynomial, exponential)</li>
                            <li>Tangent line visualization</li>
                            <li>Single & split view modes</li>
                            <li>Manual control & animation</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('calculus/differentiation_multidimension.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">Multidimensional Differentiation</div>
                    <div class="card-description">
                        Minh h·ªça 3D t∆∞∆°ng t√°c v·ªÅ ƒë·∫°o h√†m ri√™ng v√† gradient vector
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Partial derivatives ‚àÇf/‚àÇx, ‚àÇf/‚àÇy</li>
                            <li>Gradient vector ‚àáf visualization</li>
                            <li>3D surface interaction</li>
                            <li>Multiple function types (saddle, paraboloid, etc.)</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('calculus/continuity_uniform_continuity_demo.html')">
                    <div class="card-icon">üìä</div>
                    <div class="card-title">Continuity Demo</div>
                    <div class="card-description">
                        Demo t√≠nh li√™n t·ª•c v√† li√™n t·ª•c ƒë·ªÅu c·ªßa h√†m s·ªë
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Continuity definition</li>
                            <li>Uniform continuity</li>
                            <li>Epsilon-delta proofs</li>
                            <li>Function behavior</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('calculus/vector_calculus_fields.html')">
                    <div class="card-icon">üåä</div>
                    <div class="card-title">Vector Calculus Fields</div>
                    <div class="card-description">
                        Minh h·ªça t∆∞∆°ng t√°c v·ªÅ vector fields, gradient fields v√† c√°c t√≠nh ch·∫•t tr∆∞·ªùng vector
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Vector fields visualization (gradient, circulation, radial)</li>
                            <li>Divergence and curl analysis</li>
                            <li>Conservative vs non-conservative fields</li>
                            <li>Potential functions and streamlines</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('calculus/taylor_series.html')">
                    <div class="card-icon">üåä</div>
                    <div class="card-title">Taylor Series - Foundation of Approximations</div>
                    <div class="card-description">
                        üéØ <strong>Approximation Power:</strong> Taylor series = x·∫•p x·ªâ functions ph·ª©c t·∫°p b·∫±ng polynomials ƒë∆°n gi·∫£n! Activation functions (tanh, sigmoid) ƒë∆∞·ª£c approximate, numerical optimization methods, even quantum computing algorithms ƒë·ªÅu d·ª±a tr√™n Taylor expansions!
                    </div>
                    <div class="card-topics">
                        <h4>‚ö° Applications trong ML Computing:</h4>
                        <ul>
                            <li><strong>Activation approximations:</strong> Fast computation cho embedded AI systems</li>
                            <li><strong>Newton's method:</strong> Second-order optimization d·ª±a tr√™n Taylor expansion</li>
                            <li><strong>Numerical stability:</strong> Tr√°nh overflow/underflow trong exponentials</li>
                            <li><strong>Model interpretability:</strong> LIME s·ª≠ d·ª•ng linear approximations</li>
                        </ul>
                    </div>
                    <button class="open-button">Master Approximations ‚ö°</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('calculus/level_curves.html')">
                    <div class="card-icon">üó∫Ô∏è</div>
                    <div class="card-title">Level Curves - Loss Function Landscapes</div>
                    <div class="card-description">
                        üó∫Ô∏è <strong>Optimization Navigation:</strong> Level curves = contour maps c·ªßa loss functions! M·ªói ƒë∆∞·ªùng cong bi·ªÉu di·ªÖn constant loss values. Gradient descent "leo xu·ªëng" level curves ƒë·ªÉ t√¨m minimum. Essential for understanding neural network training landscapes!
                    </div>
                    <div class="card-topics">
                        <h4>üß≠ Navigation cho AI Training:</h4>
                        <ul>
                            <li><strong>Loss landscapes:</strong> Visualize neural network training difficulty</li>
                            <li><strong>Optimization paths:</strong> How gradient descent navigates contours</li>
                            <li><strong>Critical points:</strong> Local minima, saddle points trong deep learning</li>
                            <li><strong>Hyperparameter tuning:</strong> Grid search visualization with contours</li>
                        </ul>
                    </div>
                    <button class="open-button">Navigate AI Landscapes üó∫Ô∏è</button>
                </div>
            </div>
        </div>
        
        <!-- REAL ANALYSIS SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üî¨ Real Analysis & Set Theory - Mathematical Foundations</h2>
                <p>üèóÔ∏è <strong>The Mathematical Foundation:</strong> Real Analysis l√† n·ªÅn m√≥ng l√Ω thuy·∫øt cho t·∫•t c·∫£ mathematical concepts! T·ª´ convergence trong gradient descent, continuity trong loss functions, ƒë·∫øn compactness trong optimization theory - Real Analysis cung c·∫•p rigorous framework ƒë·ªÉ hi·ªÉu WHY mathematics works in ML!</p>
            </div>
            <div class="illustrations-grid">
                <div class="illustration-card" onclick="openIllustration('real-analysis/set_theory_real_analysis.html')">
                    <div class="card-icon">üî¢</div>
                    <div class="card-title">Set Theory & Real Analysis Foundations</div>
                    <div class="card-description">
                        üéØ <strong>Mathematical Rigor:</strong> Set Theory l√† ng√¥n ng·ªØ c·ªßa to√°n h·ªçc hi·ªán ƒë·∫°i! M·ªçi concept trong ML - t·ª´ probability spaces, feature spaces, ƒë·∫øn function spaces - ƒë·ªÅu ƒë∆∞·ª£c define b·∫±ng set theory. Real Analysis cung c·∫•p rigorous definitions cho limits, continuity, convergence!
                    </div>
                    <div class="card-topics">
                        <h4>üèõÔ∏è Foundational Concepts:</h4>
                        <ul>
                            <li><strong>Set Operations:</strong> Union, intersection, complement with interactive Venn diagrams</li>
                            <li><strong>Sequences & Limits:</strong> Convergence analysis for optimization algorithms</li>
                            <li><strong>Cardinality & Infinity:</strong> Countable vs uncountable sets, continuum hypothesis</li>
                            <li><strong>Topology:</strong> Open sets, closed sets, compactness in metric spaces</li>
                        </ul>
                    </div>
                    <button class="open-button">Explore Mathematical Foundations üèõÔ∏è</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('real-analysis/continuity_differentiability.html')">
                    <div class="card-icon">üìê</div>
                    <div class="card-title">Continuity & Differentiability - Œµ-Œ¥ Rigor</div>
                    <div class="card-description">
                        üîç <strong>Mathematical Precision:</strong> Œµ-Œ¥ definitions l√† heart c·ªßa mathematical rigor! Understanding WHY gradient descent converges, WHEN loss functions are smooth, HOW optimization landscapes behave - all require deep understanding c·ªßa continuity v√† differentiability theory!
                    </div>
                    <div class="card-topics">
                        <h4>‚ö° Rigorous Analysis:</h4>
                        <ul>
                            <li><strong>Œµ-Œ¥ Continuity:</strong> Interactive visualization of epsilon-delta proofs</li>
                            <li><strong>Differentiability:</strong> Difference quotient limits and derivative existence</li>
                            <li><strong>Function Pathologies:</strong> Continuous but non-differentiable functions</li>
                            <li><strong>Uniform Continuity:</strong> Heine-Cantor theorem and practical implications</li>
                        </ul>
                    </div>
                    <button class="open-button">Master Mathematical Rigor ‚ö°</button>
                </div>
            </div>
        </div>
        
        <!-- FOURIER ANALYSIS SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üåä B√†i 4: Ph√¢n T√≠ch Fourier - L√†n s√≥ng c·ªßa AI Audio/Vision</h2>
                <p>üéµ <strong>Wave Revolution:</strong> Fourier transforms l√† DNA c·ªßa signal processing! T·ª´ Siri speech recognition, Spotify audio features, ƒë·∫øn JPEG compression - t·∫•t c·∫£ ƒë·ªÅu d·ª±a tr√™n vi·ªác ph√¢n t√°ch signals th√†nh frequency components. CNNs th·∫≠m ch√≠ "h·ªçc" gi·ªëng Fourier filters!</p>
            </div>
            <div class="illustrations-grid">
                <div class="illustration-card" onclick="openIllustration('fourier-analysis/fourier.html')">
                    <div class="card-icon">üåä</div>
                    <div class="card-title">Fourier Series - Spotify's Music DNA</div>
                    <div class="card-description">
                        üéµ <strong>Music Intelligence:</strong> Spotify analyze 30 tri·ªáu b√†i h√°t b·∫±ng Fourier! Decompose audio th√†nh frequency bands ƒë·ªÉ detect genre, mood, energy level. Machine learning models d√πng spectrograms (Fourier transforms) ƒë·ªÉ classify music, speech recognition, even bird song identification!
                    </div>
                    <div class="card-topics">
                        <h4>üéº Audio AI Applications:</h4>
                        <ul>
                            <li><strong>Music recommendation:</strong> Genre classification, mood analysis, audio fingerprinting</li>
                            <li><strong>Speech processing:</strong> ASR systems, voice assistants, noise reduction</li>
                            <li><strong>Signal classification:</strong> Medical ECG analysis, earthquake detection</li>
                            <li><strong>Feature extraction:</strong> MFCCs, spectral features cho ML models</li>
                        </ul>
                    </div>
                    <button class="open-button">Decode Audio Signals üéº</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('fourier-analysis/fourier_square_wave.html')">
                    <div class="card-icon">‚¨ú</div>
                    <div class="card-title">Fourier Series - Square Wave</div>
                    <div class="card-description">
                        Minh h·ªça chu·ªói Fourier cho s√≥ng vu√¥ng v·ªõi c√°c h√†i b·∫≠c l·∫ª
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Square wave decomposition</li>
                            <li>Odd harmonics</li>
                            <li>Gibbs phenomenon</li>
                            <li>Convergence analysis</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('fourier-analysis/fourier_sawtooth.html')">
                    <div class="card-icon">üìê</div>
                    <div class="card-title">Fourier Series - Sawtooth Wave</div>
                    <div class="card-description">
                        Minh h·ªça chu·ªói Fourier cho s√≥ng xung rƒÉng c∆∞a
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Sawtooth wave analysis</li>
                            <li>All harmonics</li>
                            <li>Phase relationships</li>
                            <li>Series convergence</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('fourier-analysis/fourier_triagle.html')">
                    <div class="card-icon">üìä</div>
                    <div class="card-title">Fourier Series - Triangle Wave</div>
                    <div class="card-description">
                        Minh h·ªça chu·ªói Fourier cho s√≥ng tam gi√°c
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Triangle wave decomposition</li>
                            <li>Even function symmetry</li>
                            <li>Harmonic content</li>
                            <li>Amplitude relationships</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
            </div>
        </div>
        
        <!-- LINEAR PROGRAMMING SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üìä Quy Ho·∫°ch Tuy·∫øn T√≠nh</h2>
                <p>Thu·∫≠t to√°n Simplex, ƒë·ªëi ng·∫´u v√† b√†i to√°n t·ªëi ∆∞u tuy·∫øn t√≠nh</p>
            </div>
            <div class="illustrations-grid">
                <div class="illustration-card" onclick="openIllustration('linear-programming/linear_programming_illustration.html')">
                    <div class="card-icon">üìä</div>
                    <div class="card-title">Linear Programming</div>
                    <div class="card-description">
                        Minh h·ªça b√†i to√°n quy ho·∫°ch tuy·∫øn t√≠nh v·ªõi v√πng kh·∫£ thi v√† ƒëi·ªÉm t·ªëi ∆∞u
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Feasible region</li>
                            <li>Corner point method</li>
                            <li>Objective function</li>
                            <li>Linear constraints</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/linear_programming_demo.html')">
                    <div class="card-icon">üìà</div>
                    <div class="card-title">Linear Programming Demo</div>
                    <div class="card-description">
                        Demo quy ho·∫°ch tuy·∫øn t√≠nh v·ªõi ph∆∞∆°ng ph√°p ƒë·ªì th·ªã
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Linear objective functions</li>
                            <li>Feasible region display</li>
                            <li>Graphical method</li>
                            <li>Optimal solution finding</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/simplex_illustration.html')">
                    <div class="card-icon">üßÆ</div>
                    <div class="card-title">Simplex Algorithm</div>
                    <div class="card-description">
                        Minh h·ªça thu·∫≠t to√°n Simplex v·ªõi b·∫£ng Simplex v√† c√°c b∆∞·ªõc th·ª±c hi·ªán
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Simplex tableau</li>
                            <li>Pivot operations</li>
                            <li>Entering/leaving variables</li>
                            <li>Optimality conditions</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/simplex_algorithm_demo.html')">
                    <div class="card-icon">üìä</div>
                    <div class="card-title">Simplex Algorithm Demo</div>
                    <div class="card-description">
                        Demo thu·∫≠t to√°n Simplex v·ªõi b·∫£ng tableau t·ª´ng b∆∞·ªõc
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Simplex tableau method</li>
                            <li>Pivot operations</li>
                            <li>Variable selection</li>
                            <li>Step-by-step solving</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/simplex_optimization_demo.html')">
                    <div class="card-icon">üìä</div>
                    <div class="card-title">Simplex Optimization Demo</div>
                    <div class="card-description">
                        Demo t·ªëi ∆∞u h√≥a v·ªõi thu·∫≠t to√°n Simplex
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Simplex method steps</li>
                            <li>Basic feasible solutions</li>
                            <li>Optimality testing</li>
                            <li>Degeneracy handling</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/duality_illustration.html')">
                    <div class="card-icon">üîÑ</div>
                    <div class="card-title">Duality in Linear Programming</div>
                    <div class="card-description">
                        Minh h·ªça Primal/Dual problems v√† Duality theory
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Primal vs Dual problems</li>
                            <li>Weak and Strong duality</li>
                            <li>Complementary slackness</li>
                            <li>SVM duality</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/duality_linear_programming.html')">
                    <div class="card-icon">üîÑ</div>
                    <div class="card-title">LP Duality</div>
                    <div class="card-description">
                        Minh h·ªça ƒë·ªëi ng·∫´u trong quy ho·∫°ch tuy·∫øn t√≠nh
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Primal-dual construction</li>
                            <li>Strong duality theorem</li>
                            <li>Economic interpretation</li>
                            <li>Shadow prices</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('linear-programming/general_duality_illustration.html')">
                    <div class="card-icon">üîó</div>
                    <div class="card-title">General Duality Theory</div>
                    <div class="card-description">
                        Minh h·ªça l√Ω thuy·∫øt ƒë·ªëi ng·∫´u t·ªïng qu√°t v·ªõi Lagrangian v√† ƒëi·ªÅu ki·ªán KKT
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Lagrangian function</li>
                            <li>Primal-dual relationships</li>
                            <li>KKT conditions</li>
                            <li>Complementary slackness</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
            </div>
        </div>
        
        <!-- CONVEX OPTIMIZATION SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üî∑ B√†i 6: T·ªëi ∆Øu L·ªìi - Tr√°i tim c·ªßa Modern ML</h2>
                <p>üíé <strong>The Golden Standard:</strong> Convex optimization = guarantee t√¨m ƒë∆∞·ª£c global minimum! SVM, Logistic Regression, Lasso, Ridge - t·∫•t c·∫£ ƒë·ªÅu l√† convex problems. T·∫°i sao? V√¨ ch√∫ng ta c·∫ßn certainty trong business decisions. Hi·ªÉu convexity = master ƒë∆∞·ª£c 80% ML algorithms!</p>
            </div>
            <div class="illustrations-grid">
                <div class="illustration-card" onclick="openIllustration('convex-analysis/convex_sets_illustration.html')">
                    <div class="card-icon">üî∑</div>
                    <div class="card-title">Convex Sets - Foundation of Reliable AI</div>
                    <div class="card-description">
                        üõ°Ô∏è <strong>Reliability Guarantee:</strong> Convex sets ƒë·∫£m b·∫£o m·ªçi local optimum ƒë·ªÅu l√† global optimum! SVM feasible region, regularization constraints (L1 ball, L2 ball) ƒë·ªÅu l√† convex sets. Netflix, Google Ads optimization ƒë·ªÅu leverage convex constraints ƒë·ªÉ ƒë·∫£m b·∫£o stable solutions!
                    </div>
                    <div class="card-topics">
                        <h4>üèõÔ∏è Convex Sets trong Production ML:</h4>
                        <ul>
                            <li><strong>SVM margin constraints:</strong> Maximum margin = convex optimization problem</li>
                            <li><strong>Regularization regions:</strong> L1/L2 balls, elastic net constraints</li>
                            <li><strong>Probability simplex:</strong> Softmax outputs, mixture model weights</li>
                            <li><strong>Semi-definite constraints:</strong> Covariance matrices, kernel learning</li>
                        </ul>
                    </div>
                    <button class="open-button">Build Reliable Models üõ°Ô∏è</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('convex-analysis/convex_optimization_illustration.html')">
                    <div class="card-icon">üî∑</div>
                    <div class="card-title">Convex Optimization</div>
                    <div class="card-description">
                        Minh h·ªça b√†i to√°n t·ªëi ∆∞u l·ªìi v√† ƒëi·ªÅu ki·ªán t·ªëi ∆∞u b·∫≠c nh·∫•t
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Convex vs non-convex functions</li>
                            <li>First-order optimality conditions</li>
                            <li>Convexity testing</li>
                            <li>Constrained optimization</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('convex-analysis/convex_demo.html')">
                    <div class="card-icon">üî∑</div>
                    <div class="card-title">Convex Functions Demo</div>
                    <div class="card-description">
                        Demo t∆∞∆°ng t√°c v·ªÅ h√†m l·ªìi v√† t√≠nh ch·∫•t t·ªëi ∆∞u
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Convex function properties</li>
                            <li>Jensen's inequality</li>
                            <li>Global minimum</li>
                            <li>Convexity testing</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('convex-analysis/convex_constraints_feasible_regions.html')">
                    <div class="card-icon">üéØ</div>
                    <div class="card-title">Convex Constraints & Feasible Regions</div>
                    <div class="card-description">
                        Minh h·ªça t∆∞∆°ng t√°c v·ªÅ r√†ng bu·ªôc l·ªìi v√† v√πng kh·∫£ thi trong t·ªëi ∆∞u h√≥a c√≥ r√†ng bu·ªôc
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Interactive constraint definition and feasible region visualization</li>
                            <li>Linear programming with equality and inequality constraints</li>
                            <li>Vertex enumeration and extreme points identification</li>
                            <li>Preset problems: bounded, unbounded, infeasible cases</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/lagrangian_kkt_conditions.html')">
                    <div class="card-icon">‚öñÔ∏è</div>
                    <div class="card-title">KKT Conditions - The SVM Secret</div>
                    <div class="card-description">
                        üéØ <strong>SVM's Mathematical Core:</strong> KKT conditions explain t·∫°i sao ch·ªâ support vectors quan tr·ªçng! Lagrange multipliers Œ±·µ¢ = 0 cho non-support vectors, Œ±·µ¢ > 0 cho support vectors. Complementary slackness = key insight behind SVM efficiency v√† sparse solutions!
                    </div>
                    <div class="card-topics">
                        <h4>‚öñÔ∏è KKT trong Production Algorithms:</h4>
                        <ul>
                            <li><strong>SVM training:</strong> SMO algorithm leverage KKT conditions</li>
                            <li><strong>Portfolio optimization:</strong> Budget constraints, no-short selling</li>
                            <li><strong>Neural Architecture Search:</strong> Resource-constrained model design</li>
                            <li><strong>Fairness constraints:</strong> ML fairness th√¥ng qua constrained optimization</li>
                        </ul>
                    </div>
                    <button class="open-button">Decode SVM Magic ‚öñÔ∏è</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('convex-analysis/convex_conjugates_duality.html')">
                    <div class="card-icon">üîÑ</div>
                    <div class="card-title">Duality - Two Sides of ML Coin</div>
                    <div class="card-description">
                        üí∞ <strong>Algorithmic Breakthrough:</strong> Duality theory = solve difficult primal problems qua easier dual problems! SVM dual formulation cho ph√©p kernel trick. Fenchel duality gi·∫£i th√≠ch t·∫°i sao regularization works. Hi·ªÉu duality = unlock advanced ML algorithms!
                    </div>
                    <div class="card-topics">
                        <h4>ü™ô Duality trong Advanced ML:</h4>
                        <ul>
                            <li><strong>SVM kernel trick:</strong> Dual formulation enables infinite-dimensional feature spaces</li>
                            <li><strong>Fenchel-Rockafellar duality:</strong> Foundation of proximal methods</li>
                            <li><strong>Lagrangian relaxation:</strong> Large-scale optimization decomposition</li>
                            <li><strong>GANs mathematical foundation:</strong> Minimax games v√† duality theory</li>
                        </ul>
                    </div>
                    <button class="open-button">Flip the ML Coin ü™ô</button>
                </div>
            </div>
        </div>
        
        <!-- GENERAL OPTIMIZATION SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üéØ B√†i 6: T·ªëi ∆Øu H√≥a T·ªïng Qu√°t - Deep Learning Revolution</h2>
                <p>üöÄ <strong>Non-Convex Mastery:</strong> Deep learning = non-convex optimization! Gradient descent variants (Adam, RMSprop), loss landscapes v·ªõi millions local minima, second-order methods, stochastic optimization. ƒê√¢y l√† frontier of AI research - n∆°i mathematics meets engineering marvels!</p>
            </div>
            <div class="illustrations-grid">
                
                <div class="illustration-card" onclick="openIllustration('optimization/gradient_descent_illustration.html')">
                    <div class="card-icon">üìâ</div>
                    <div class="card-title">Gradient Descent - The Learning Algorithm</div>
                    <div class="card-description">
                        üß† <strong>Universal Learning Principle:</strong> M·ªçi neural network ƒë·ªÅu h·ªçc th√¥ng qua gradient descent! T·ª´ simple linear regression ƒë·∫øn GPT models v·ªõi 175 billion parameters. Learning rate scheduling, momentum, adaptive methods - t·∫•t c·∫£ ƒë·ªÅu variations c·ªßa gradient descent core idea!
                    </div>
                    <div class="card-topics">
                        <h4>üî• GD Variants trong Deep Learning:</h4>
                        <ul>
                            <li><strong>SGD + Momentum:</strong> Accelerated convergence, escape saddle points</li>
                            <li><strong>Adam optimizer:</strong> Adaptive learning rates, default choice cho deep learning</li>
                            <li><strong>Learning rate scheduling:</strong> Warmup, cosine decay, step decay</li>
                            <li><strong>Gradient clipping:</strong> Stable training cho RNNs, Transformers</li>
                        </ul>
                    </div>
                    <button class="open-button">Train Like GPT üß†</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/gradient_descent_demo.html')">
                    <div class="card-icon">üìâ</div>
                    <div class="card-title">Gradient Descent Demo</div>
                    <div class="card-description">
                        Demo t∆∞∆°ng t√°c thu·∫≠t to√°n Gradient Descent
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Step size selection</li>
                            <li>Convergence visualization</li>
                            <li>Different objective functions</li>
                            <li>Learning rate effects</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/newton_method_illustration.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">Newton's Method</div>
                    <div class="card-description">
                        Minh h·ªça Newton's Method v√† so s√°nh v·ªõi Gradient Descent
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Hessian matrix</li>
                            <li>Second-order methods</li>
                            <li>Quadratic convergence</li>
                            <li>Method comparison</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/newtons_method_demo.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">Newton's Method Demo</div>
                    <div class="card-description">
                        Demo ph∆∞∆°ng ph√°p Newton v·ªõi Hessian matrix
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Second-order methods</li>
                            <li>Hessian computation</li>
                            <li>Quadratic convergence</li>
                            <li>Line search</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/quasi_newton_illustration.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">Quasi-Newton Methods</div>
                    <div class="card-description">
                        Minh h·ªça Quasi-Newton methods v√† so s√°nh v·ªõi Newton's method
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>BFGS update formula</li>
                            <li>Hessian approximation</li>
                            <li>Secant equation</li>
                            <li>Computational efficiency</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/quasi_newton_demo.html')">
                    <div class="card-icon">üî¨</div>
                    <div class="card-title">Quasi-Newton Demo</div>
                    <div class="card-description">
                        Demo c√°c ph∆∞∆°ng ph√°p Quasi-Newton (BFGS, L-BFGS)
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>BFGS updates</li>
                            <li>Hessian approximation</li>
                            <li>Limited memory BFGS</li>
                            <li>Secant conditions</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/subgradient_illustration.html')">
                    <div class="card-icon">üìê</div>
                    <div class="card-title">Subgradient Method</div>
                    <div class="card-description">
                        Minh h·ªça ph∆∞∆°ng ph√°p Subgradient v√† so s√°nh v·ªõi Gradient Descent
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Subgradient definition</li>
                            <li>Non-differentiable functions</li>
                            <li>Subgradient vs gradient</li>
                            <li>Step size selection</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/subgradient_demo.html')">
                    <div class="card-icon">üìê</div>
                    <div class="card-title">Subgradient Method Demo</div>
                    <div class="card-description">
                        Demo ph∆∞∆°ng ph√°p subgradient cho h√†m kh√¥ng kh·∫£ vi
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Subgradient computation</li>
                            <li>Non-differentiable optimization</li>
                            <li>Step size rules</li>
                            <li>Convergence analysis</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/proximal_gradient_illustration.html')">
                    <div class="card-icon">üéØ</div>
                    <div class="card-title">Proximal Gradient Descent</div>
                    <div class="card-description">
                        Minh h·ªça Proximal Gradient Descent v√† Proximal Operator
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Proximal operator</li>
                            <li>Soft thresholding</li>
                            <li>Lasso regression</li>
                            <li>Non-smooth optimization</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/proximal_gradient_demo.html')">
                    <div class="card-icon">üéØ</div>
                    <div class="card-title">Proximal Gradient Demo</div>
                    <div class="card-description">
                        Demo thu·∫≠t to√°n Proximal Gradient cho t·ªëi ∆∞u composite
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Proximal operators</li>
                            <li>Composite optimization</li>
                            <li>ISTA/FISTA algorithms</li>
                            <li>Soft thresholding</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/proximal_operator_demo.html')">
                    <div class="card-icon">üéØ</div>
                    <div class="card-title">Proximal Operator Demo</div>
                    <div class="card-description">
                        Demo c√°c proximal operator cho c√°c h√†m kh√°c nhau
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Proximal operator definition</li>
                            <li>Soft thresholding</li>
                            <li>Projection operators</li>
                            <li>Moreau envelope</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/stochastic_gradient_illustration.html')">
                    <div class="card-icon">üé≤</div>
                    <div class="card-title">Stochastic Gradient Descent</div>
                    <div class="card-description">
                        Minh h·ªça SGD, Batch GD v√† Mini-batch GD
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Batch vs SGD</li>
                            <li>Mini-batch GD</li>
                            <li>Learning rate scheduling</li>
                            <li>Noise and convergence</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/stochastic_gradient_demo.html')">
                    <div class="card-icon">üé≤</div>
                    <div class="card-title">Stochastic Gradient Demo</div>
                    <div class="card-description">
                        Demo SGD v√† c√°c bi·∫øn th·ªÉ v·ªõi noise v√† mini-batch
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>SGD vs batch GD</li>
                            <li>Mini-batch strategies</li>
                            <li>Learning rate scheduling</li>
                            <li>Variance reduction</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/kkt_conditions_demo.html')">
                    <div class="card-icon">‚úÖ</div>
                    <div class="card-title">KKT Conditions Demo</div>
                    <div class="card-description">
                        Demo ƒëi·ªÅu ki·ªán KKT cho t·ªëi ∆∞u c√≥ r√†ng bu·ªôc
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Karush-Kuhn-Tucker conditions</li>
                            <li>Constrained optimization</li>
                            <li>Lagrange multipliers</li>
                            <li>Complementary slackness</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/parallel_computing_illustration.html')">
                    <div class="card-icon">üöÄ</div>
                    <div class="card-title">Parallel Computing in ML</div>
                    <div class="card-description">
                        Minh h·ªça c√°c lo·∫°i parallelism v√† performance metrics
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Data vs Model parallelism</li>
                            <li>Speedup v√† Efficiency</li>
                            <li>Communication overhead</li>
                            <li>Worker coordination</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/Nelder-Mead.html')">
                    <div class="card-icon">üîç</div>
                    <div class="card-title">Nelder-Mead Algorithm</div>
                    <div class="card-description">
                        Minh h·ªça thu·∫≠t to√°n Nelder-Mead simplex method
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Simplex operations</li>
                            <li>Reflection, expansion, contraction</li>
                            <li>Direct search method</li>
                            <li>Derivative-free optimization</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('optimization/loss_function_landscapes.html')">
                    <div class="card-icon">üèîÔ∏è</div>
                    <div class="card-title">Loss Landscapes - The AI Training Terrain</div>
                    <div class="card-description">
                        üèîÔ∏è <strong>Deep Learning Geography:</strong> Neural network loss landscapes c√≥ millions peaks v√† valleys! Visualize why deep learning works: good local minima everywhere, flat minima generalize better. Explore famous landscapes: Rosenbrock (optimization benchmark), loss surfaces c·ªßa real neural networks!
                    </div>
                    <div class="card-topics">
                        <h4>üó∫Ô∏è Navigate AI Training Terrain:</h4>
                        <ul>
                            <li><strong>Loss surface analysis:</strong> Visualize why ResNets train better than plain networks</li>
                            <li><strong>Optimizer comparison:</strong> SGD vs Adam vs RMSprop trajectories</li>
                            <li><strong>Learning rate effects:</strong> Too high = divergence, too low = slow convergence</li>
                            <li><strong>Saddle points:</strong> Dominant obstacle trong high-dimensional optimization</li>
                        </ul>
                    </div>
                    <button class="open-button">Explore AI Terrain üó∫Ô∏è</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('machine-learning/neural_network_illustration.html')">
                    <div class="card-icon">üß†</div>
                    <div class="card-title">Neural Networks - The Universal Approximator</div>
                    <div class="card-description">
                        üåü <strong>The AI Revolution:</strong> Neural networks = universal function approximators! T·ª´ recognizing cats trong photos ƒë·∫øn generating human-like text. Forward pass = predictions, backpropagation = learning t·ª´ mistakes. T·∫•t c·∫£ ch·ªâ l√† matrix multiplications v√† non-linear activations!
                    </div>
                    <div class="card-topics">
                        <h4>üåü Neural Network Revolution:</h4>
                        <ul>
                            <li><strong>Universal approximation:</strong> Any continuous function can be approximated</li>
                            <li><strong>Backpropagation:</strong> Chain rule enables deep learning revolution</li>
                            <li><strong>Activation functions:</strong> ReLU, Sigmoid, Tanh - sources of non-linearity</li>
                            <li><strong>Architecture evolution:</strong> MLPs ‚Üí CNNs ‚Üí RNNs ‚Üí Transformers</li>
                        </ul>
                    </div>
                    <button class="open-button">Build Your First AI üåü</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('machine-learning/lasso_regression_demo.html')">
                    <div class="card-icon">üéØ</div>
                    <div class="card-title">Lasso Regression Demo</div>
                    <div class="card-description">
                        Demo h·ªìi quy Lasso v·ªõi regularization L1
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>L1 regularization</li>
                            <li>Feature selection</li>
                            <li>Sparsity induction</li>
                            <li>Regularization path</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
                
                <div class="illustration-card" onclick="openIllustration('machine-learning/linear_regression_demo.html')">
                    <div class="card-icon">üìâ</div>
                    <div class="card-title">Linear Regression Demo</div>
                    <div class="card-description">
                        Demo h·ªìi quy tuy·∫øn t√≠nh v·ªõi least squares method
                    </div>
                    <div class="card-topics">
                        <h4>Kh√°i ni·ªám ch√≠nh:</h4>
                        <ul>
                            <li>Least squares method</li>
                            <li>Best-fit line calculation</li>
                            <li>Interactive data points</li>
                            <li>Statistical measures</li>
                        </ul>
                    </div>
                    <button class="open-button">M·ªü minh h·ªça</button>
                </div>
            </div>
        </div>
        
        <!-- MACHINE LEARNING ALGORITHMS SECTION -->
        <div class="topic-section">
            <div class="topic-header">
                <h2>üß† Machine Learning</h2>
                <p>Regression, classification algorithms v√† advanced optimization methods</p>
            </div>
            <div class="illustrations-grid">
                <!-- The rest of ML visualizations will continue in the existing sections -->
    </div>
        
        <div class="intro-section">
            <h2>üéØ Data Science Learning Roadmap</h2>
            <p>
                <strong>üèÅ Suggested Learning Path:</strong><br>
                <strong>Week 1-2: Foundation</strong> ‚Üí Linear Algebra (vectors, matrices) ‚Üí Understand data representation<br>
                <strong>Week 3-4: Core Skills</strong> ‚Üí Calculus (gradients) ‚Üí Master optimization fundamentals<br>  
                <strong>Week 5-6: Advanced</strong> ‚Üí Convex Optimization ‚Üí Learn SVM, logistic regression math<br>
                <strong>Week 7-8: Deep Learning</strong> ‚Üí General Optimization ‚Üí Understand neural network training<br>
                <strong>Week 9+: Specialization</strong> ‚Üí Fourier Analysis, Linear Programming ‚Üí Domain-specific applications
            </p>
            
            <div style="background: rgba(255, 215, 0, 0.1); padding: 20px; border-radius: 10px; margin: 20px 0;">
                <h3>üíº Career Connections</h3>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                    <div>
                        <h4>üöÄ Tech Giants</h4>
                        <p><strong>Google:</strong> PageRank (eigenvectors), Search algorithms<br>
                        <strong>Netflix:</strong> SVD for recommendations<br>
                        <strong>Tesla:</strong> Computer vision, optimization for autopilot</p>
                    </div>
                    <div>
                        <h4>üí∞ Industry Applications</h4>
                        <p><strong>Finance:</strong> Portfolio optimization, risk modeling<br>
                        <strong>Healthcare:</strong> Medical imaging, drug discovery<br>
                        <strong>E-commerce:</strong> Demand forecasting, price optimization</p>
                    </div>
                </div>
            </div>
            
            <div class="features">
                <div class="feature">
                    <div class="feature-icon">üéÆ</div>
                    <h3>Interactive Learning</h3>
                    <p>Click ‚Üí Explore ‚Üí Adjust parameters ‚Üí See real-time results ‚Üí Build intuition!</p>
                </div>
                <div class="feature">
                    <div class="feature-icon">üî¨</div>
                    <h3>Research Connection</h3>
                    <p>Each lesson connects to cutting-edge research papers v√† industry breakthroughs</p>
                </div>
                <div class="feature">
                    <div class="feature-icon">üèÜ</div>
                    <h3>Interview Prep</h3>
                    <p>Master concepts commonly asked trong data scientist technical interviews</p>
                </div>
            </div>
        </div>
    </div>

    <script>
        function openIllustration(filename) {
            window.open(filename, '_blank');
        }
        
        // Add some interactive effects
        document.querySelectorAll('.illustration-card').forEach(card => {
            card.addEventListener('mouseenter', function() {
                this.style.background = 'rgba(255, 255, 255, 0.2)';
            });
            
            card.addEventListener('mouseleave', function() {
                this.style.background = 'rgba(255, 255, 255, 0.15)';
            });
        });
        
        // Add smooth scrolling
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html> 